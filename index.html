<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  <meta name="author" content="Ukcheol Shin">
  <meta name="description" content="Ph.D Candidate">
  <link rel="alternate" hreflang="en-us" href="/">
  <meta name="theme-color" content="#2962ff">
   
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous"> 
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
  
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  <link rel="stylesheet" href="/css/academic.css">
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Ukcheol Shin">
  <link rel="manifest" href="/index.webmanifest">
  <!-- <link rel="icon" type="image/png" href="/images/icon_hu2065e2b3e3f9ec61858579cdb7515c4d_74858_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu2065e2b3e3f9ec61858579cdb7515c4d_74858_192x192_fill_lanczos_center_2.png"> -->

  <link rel="canonical" href="/">
  <meta property="twitter:card" content="summary">
  <meta property="og:site_name" content="Ukcheol Shin">
  <meta property="og:url" content="/">
  <meta property="og:title" content="Ukcheol Shin">
  <meta property="og:description" content="Assistant Professor"><meta property="og:image" content="img/map[gravatar:%!s(bool=false) shape:circle]">
  <meta property="twitter:image" content="img/map[gravatar:%!s(bool=false) shape:circle]"><meta property="og:locale" content="en-us">
  

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebSite",
  "potentialAction": {
    "@type": "SearchAction",
    "target": "/?q={search_term_string}",
    "query-input": "required name=search_term_string"
  },
  "url": "/"
}
</script>
  <title>Ukcheol Shin</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
      </div>

    </section>
  </div>
</aside>


<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Ukcheol Shin</a>
    </div>
    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>

    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Ukcheol Shin</a>
    </div>

    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content"> 
      <ul class="navbar-nav d-md-inline-flex">
        <li class="nav-item">
          <a class="nav-link " href="/#about" data-target="#about"><span>Home</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/#papers" data-target="#papers"><span>Publications</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/#awards" data-target="#awards"><span>Awards &amp; Honors</span></a>
        </li>

        <li class="nav-item">
          <a class="nav-link " href="/#activities" data-target="#activities"><span>Activities</span></a>
        </li>


      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>

    </ul>

  </div>
</nav>


<span class="js-widget-page d-none"></span>

  <section id="about" class="home-section wg-about   " style="padding: 30px 0 20px 0;" >
    <div class="container">

<div class="row">
  <div class="col-12 col-lg-4">
    <div id="profile">
      <img class="avatar avatar-circle" src="/authors/admin/profile1.jpg" alt="Avatar">
      <div class="portrait-title">
        <h2>Ukcheol Shin</h2>
        <h3>Postdoctoral Fellow</h3>
        
        <h3>
          <a href="https://www.ri.cmu.edu/" target="_blank" rel="noopener">
          <span>Robotics Institute, CMU</span>
          </a>
        </h3>
      </div>
      
      <td>
      <br>
      <a href="cv/UkcheolShin_CV.pdf"><b>CV</b></a>
        | <a href="https://scholar.google.co.kr/citations?user=ZvxI80EAAAAJ&hl=ko"><b>Google Scholar</b></a>
        | <a href="https://github.com/UkcheolShin"><b>Github</b></a>
      <br>
      <br>
      </td>
     <!--  <ul class="network-icon" aria-hidden="true">    
        <li>
          <a href="https://scholar.google.com/citations?user=mHpN1xoAAAAJ&amp;hl=ko&amp;oi=ao" target="_blank" rel="noopener">
            <i class="ai ai-google-scholar big-icon"></i>
          </a>
        </li>  

        <li>
          <a href="https://github.com/mcahny" target="_blank" rel="noopener">
            <i class="fab fa-github big-icon"></i>
          </a>
        </li>

        <li>
          <a href="cv/dahun_cv_2020Mar.pdf" target="_blank" rel="noopener">
            <i class="fas fa-file-word big-icon"></i>
          </a>
        </li>

      </ul> -->
    </div>
  </div>
  <div class="col-12 col-lg-8">

    
<br>
<p> I am a postdoctoral researcher at Robotics Institute, <a href="https://www.ri.cmu.edu/ri-people/ukcheol-shin/">CMU</a>. 

I obtained my Ph.D. and M.S. at <a href="https://kaist.ac.kr/" target="_blank" rel="noopener">KAIST</a>, advised by Professor <a href="http://rcv.kaist.ac.kr">In So Kweon</a>.
I am a recipient of the Best Student Paper Award from <a href="https://wacv2023.thecvf.com/node/174" target="_blank" rel="noopener">WACV 2023</a>.

<p> My research focuses on developing a robust physical AI that can perceive, understand, and navigate the dynamic world in challenging conditions, with a specific interest in spatial/semantic perception in extreme conditions, self-supervised learning, deep reinforcement learning, multi-sensor fusion, and vision-language navigation/manipulation. 
I am interested in the following areas, but also open to other explorable/challenging domains.

<ul>
<li> Unsupervised and Self-supervised 3D Geometry (depth, optical flow, odometry, SLAM) </li>
<li> 3D Geometry in Challenging Conditions (rainy, snowy, over-exposed, low-lighted, etc)</li>
<li> Learning Representation from Multi-modal Sensor and Self-supervision</li>
<li> Deep Reinforcement Learning for Robot/Sensor (legged robot, manipulator, camera)</li>
<li> Vision-Language Navigation/Manipulation</li>
</ul>
</p>

    <div class="row">

      <div class="col-md-6">
        <h3>Contact</h3>
        <ul class="ul-contact fa-ul">
          <li>          
            <i class="fa-li fas fa-envelope"></i>
            <div class="description">
              <p class="course">shinwc159 [at] gmail.com</p>
              <p class="course">ushin [at] andrew.cmu.edu</p>
            </div>
          </li>
          <li>  
            <i class="fa-li fas fa-map-marker"></i>
            <div class="description">
              <p class="course">Robotics Institute, 1723 Murray Ave, Pittsburgh, PA 15217</p>
            </div> 
          </li>
          <!-- <li>  
            <i class="fa-li fas fa-phone-alt"></i>
            <div class="description">
              <p class="course">(&#43;82)-42-350-3579</p>
            </div> 
          </li>  -->         
        </ul>        
      </div>

      <div class="col-md-6">
        <h3>Education</h3>
        <ul class="ul-edu fa-ul">
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">Ph.D. in EE, KAIST, 2023</p>
              <!-- <p class="course">Ph.D. in Electrical Engineering, KAIST, 2023</p> -->
              <!-- <p class="institution">Korea Advanced Institute of Science and Technology (KAIST), Korea</p> -->
              <!-- <p class="institution">KAIST, Korea</p> -->
              <!-- <p class="institution">Dissertation: "Self-supervised 3D Geometric Perception in Adverse Real-world Environment"</p>-->
              <p class="institution">Advisor: Prof. In So Kweon</p>
            </div>
          </li>
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">M.S. in EE, KAIST, 2019</p> 
              <!-- <p class="course">M.S. in Electrical Engineering, KAIST, 2019</p> -->
              <!-- <p class="institution">Korea Advanced Institute of Science and Technology (KAIST), Korea</p> -->
              <!-- <p class="institution">KAIST, Korea</p> -->
              <!--<p class="institution">Thesis: "Noise-Aware Camera Exposure Control for Robust Robot Vision"</p>-->
              <p class="institution">Advisor: Prof. In So Kweon</p>
            </div>
          </li>
          
          <li>
            <i class="fa-li fas fa-graduation-cap"></i>
            <div class="description">
              <p class="course">B.S. in EE, SNUST, 2017</p> 
              <!-- <p class="course">B.S. in Electrical and Information Engineering, SNUST, 2017</p> -->
              <!-- <p class="institution">Seoul National University of Science and Technology, Korea</p> -->
              <!-- <p class="institution">SNUST, Korea</p> -->
              <!--<p class="institution">Project: "Real-Time Ethernet Protocol based Omni Directional Mobile Robot"</p>-->
              <p class="institution">Advisor: Prof. Byoung Wook Choi</p>
            </div>
          </li>

        </ul>
      </div>
    </div>
  </div>
</div>

    </div>
  </section>



<section id="experience" class="home-section wg-blank   " style="padding: 10px 0 10px 0;" >
<div class="container">

<div class="row">
    <div class="col-lg-12">
    <h1>Research Experiences</h1>
      <!-- <h3 id="experience">Research Experiences</h3> -->
<ul>

<li> <div style="float:left"><b>Carnegie Mellon University</b>, Pittsburgh, United States</div> <div style="float:right">Aug. 2023 - Present </div> <br>
    Postdoctoral Fellow (Postdoc), Bot Intelligence Group (Advisor: Jean Oh). <br>
</li>

<li> <div style="float:left"><b>Korea Advanced Institute of Science and Technology</b>, Daejeon, Korea</div> <div style="float:right">Sep. 2019 - Aug. 2023</div> <br>
    Research Assistant (Ph.D.), Robotics and Computer Vision Lab (Advisor: In So Kweon). <br>
    Research topics : Self-supervised learning, 3D Geometry, Thermal camera, Reinforcement Learning.
</li>


<li> <div style="float:left"><b>Korea Advanced Institute of Science and Technology</b>, Daejeon, Korea</div> <div style="float:right">Sep. 2017 - Aug. 2019</div> <br>
    Research Assistant (M.S.), Robotics and Computer Vision Lab (Advisor: In So Kweon). <br>
    Research topics : Camera Exposure Control, Low-level Vision, 3D Geometry.
</li>

<li> <div style="float:left"><b>Seoul National University of Science and Technology</b>, Seoul, Korea</div> <div style="float:right">Mar. 2015 - Jun. 2017</div> <br>
    Research Intern, Embedded System Lab (Advisor: Byoung Wook Choi). <br>
    Research topics : Embedded Linux, Real-time Operating System, Real-time Ethernet Protocol.
</li>

</ul>
    </div>
</div>
    </div>
  </section>

  <section id="papers" class="home-section wg-papers   " style="padding: 20px 0 20px 0;" >
    <div class="container">
      
<div class="row">
    <div class="col-lg-12">
      <h1>Publications</h1>
    </div>
    <div class="row">
        <ul class="ul-papers">

        <li>
            <div class="img">
              <img src="/papers/images/2025_CoPS.gif" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">All-day Depth Completion via Thermal-LiDAR Fusion</p>
            <p class="authors"> Janghyun Kim, Minseong Kweon, Jinsun Park*, <b>Ukcheol Shin*</b> (*Equal Corresponding) </p>
            <p class="venue"> Under-review, 2025 <span style="font-weight:normal"></span></p>
            <p class="venue"> <span style="font-weight:normal">Short version at</span> <a href="https://sites.google.com/view/tiro25/" style="color: black; text-decoration: underline;">'Thermal Infrared in Robotics'</a> workshop @ ICRA 2025 </p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2504.02356"> paper</a> /
                <a href=""> code</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2025_DeepThermal.jpg" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Deep Depth Estimation from Thermal Image: Dataset, Benchmark, and Challenges</p>
            <p class="authors"> <b>Ukcheol Shin</b>,  Jinsun Park </p>
            <p class="venue"> Under-review, 2025 <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2503.22060"> paper</a> /
                <a href="https://github.com/UkcheolShin/SupDepth4Thermal"> code</a> 
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2025_SF_VO.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">SF-VO: Self-Supervised Few-Shot Adaptation for Visual Odometry</p>
            <p class="authors"> Junhee Lee, Jean Oh, <b>Ukcheol Shin*</b>,  Kyungdon Joo* (*Equal Corresponding)</p>
            <p class="venue"> Under-review, 2025 <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href=""> paper</a> /
                <a href=""> code</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2024_VPOcc.gif" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">VPOcc: Exploiting Vanishing Point for Monocular 3D Semantic Occupancy Prediction</p>
            <p class="authors"> Junsu Kim, Junhee Lee, <b>Ukcheol Shin</b>, Jean Oh, Kyungdon Joo </p>
            <p class="venue"> Under-review, 2025 <span style="font-weight:normal"></span></p>
            <p class="venue"> <span style="font-weight:normal">Short version at</span> <a href="https://drivex-workshop.github.io/" style="color: black; text-decoration: underline;">'Foundation Models for V2X-based Cooperative Autonomous Driving'</a> workshop @ CVPR 2025 </p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2408.03551"> paper</a> /
                <a href="https://vision3d-lab.github.io/vpocc"> project</a> /
                <a href="https://github.com/joonsu0109gh/VPOcc"> code</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2024_BridgeDepth.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Bridging Spectral-wise and Multi-spectral Depth Estimation via Geometry-guided Contrastive Learning</p>
            <p class="authors"> <b>Ukcheol Shin</b>, Kyunghyun Lee, and Jean Oh </p>
            <p class="venue"> IEEE International Conference on Robotics and Automation (<b>ICRA</b>), 2025 <span style="font-weight:bolder;color:#BB2222">Oral</span> <span style="font-weight:normal"> </span></p>   
            <p class="resources">
              [
                <a href="https://www.arxiv.org/abs/2503.00793"> paper</a> /
                <a href="https://github.com/UkcheolShin/BridgeMultiSpectralDepth"> code</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2024_4DFlow.gif" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Flow4D: Leveraging 4D Voxel Network for LiDAR Scene Flow Estimatoin</p>
            <p class="authors"> Jaeyeul Kim*, Jungwan Woo*, <b>Ukcheol Shin</b>, Jean Oh, and Sunghoon Im (*Equal Contribution)</p>
            <p class="venue"> IEEE Robotics and Automation Letters (<b>RA-L</b>), 2025 <span style="font-weight:normal"></span></p>
            <p class="venue"> <span style="font-weight:bolder;color:#BB2222">1st Place Award</span>, CVPR 2024 Autonomous Driving Workshop 'Argoverse Scene Flow Challenge' <span style="font-weight:normal"> </span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2407.07995"> paper</a> /
                <a href="https://github.com/dgist-cvlab/Flow4D"> code</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2024_FireStereo.gif" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">FIReStereo: Forest InfraRed Stereo Dataset for UAS Depth Perception in Visually Degraded Environments</p>
            <p class="authors"> Devansh Dhrafan*, Yifei Liu*, Andrew Jong, <b>Ukcheol Shin</b>, Yao He, Tyler Harp, Yaoyu Hu, Jean Oh, Sebastian Scherer (*Equal Contribution)</p>
            <p class="venue"> IEEE Robotics and Automation Letters (<b>RA-L</b>), 2025 <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2409.07715"> paper</a> / 
                <a href="https://firestereo.github.io/"> project</a> /
                <a href="https://github.com/firestereo/firestereo "> code</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2024_ACCV_MultiCostVolume.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Exploiting Cross-modal Cost Volume for Multi-sensor Depth Estimation</p>
            <p class="authors"> Janghyun Kim, <b>Ukcheol Shin</b>, Seokyong Heo, Jinsun Park</p>
            <p class="venue"> Asian Conference on Computer Vision (<b>ACCV</b>), 2024 <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://openaccess.thecvf.com/content/ACCV2024/html/Kim_Exploiting_Cross-modal_Cost_Volume_for_Multi-sensor_Depth_Estimation_ACCV_2024_paper.html"> paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2024_IROS_LiDARDG.gif" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Density-aware Domain Generalization for LiDAR Semantic Segmentation</p>
            <p class="authors"> Jaeyeul Kim*, Jungwan Woo*, <b>Ukcheol Shin</b>, Jean Oh, and Sunghoon Im (*Equal Contribution)</p>
            <p class="venue"> IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2024 <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://ieeexplore.ieee.org/document/10801829"> paper</a> 
              ]
            </p> 
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2024_CVPR_DRL_AE.gif" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Learning to Control Camera Exposure via Reinforcement Learning</p>
            <p class="authors"> Kyunghyun Lee*, <b>Ukcheol Shin*</b>, Byeong-Uk Lee (* equal contribution)</p>
            <p class="venue"> Proceedings of the IEEE conference on computer vision and pattern recognition (<b>CVPR</b>), 2024 <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2404.01636"> paper</a> /
                <a href="https://sites.google.com/view/drl-ae"> project</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2024_ICRA_DRL_walk.gif" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Learning Quadrupedal Locomotion with Impaired Joints Using Random Joint Masking</p>
            <p class="authors"> Mincheol Kim, <b>Ukcheol Shin</b>, Jung-Yup Kim</p>
            <p class="venue"> IEEE International Conference on Robotics and Automation (<b>ICRA</b>), 2024 <span style="font-weight:bolder;color:#BB2222">Oral</span> <span style="font-weight:normal"> </span></p>   
            <p class="venue"> <span style="font-weight:bolder;color:#BB2222">Media coverage: covered by <a href="https://spectrum.ieee.org/video-friday-robots-with-knives"> IEEE Spectrum</a> </span> <span style="font-weight:normal"> </span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2403.00398"> paper</a> /
                <a href="https://sites.google.com/view/learning-impaired-joints-loco"> project</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2023_CRM_RGBTSemSeg.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Complementary Random Masking for RGB-T Semantic Segmentation</p>
            <p class="authors"> <b>Ukcheol Shin</b>, Kyunghyun Lee, In So Kweon, Jean Oh</p>
            <p class="venue"> IEEE International Conference on Robotics and Automation (<b>ICRA</b>), 2024 <span style="font-weight:bolder;color:#BB2222">Oral</span> <span style="font-weight:normal"> </span></p>   
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2303.17386"> paper</a> /
                <a href="https://github.com/UkcheolShin/CRM_RGBTSeg"> code</a> /
                <a href="https://sites.google.com/view/crm4rgbtseg/"> project</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2024_3DV_Nerf.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Stable Surface Regularization for Fast Few-Shot NeRF</p>
            <p class="authors"> ByeongIn Joung, Byeong-Uk Lee, Jaesung Choe, <b>Ukcheol Shin</b>, Minjun Kang, Taeyeop Lee, In-So Kweon, Kuk-Jin Yoon
            <p class="venue"> International Conference on 3D Vision (<b>3DV</b>), 2024 <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href=""> paper</a> 
              ]
            </p>
          </div>
        </li>
  
        <li>
            <div class="img">
              <img src="/papers/images/2023_Joint_self_adv_for_Tdepth.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Joint Self-supervised Learning and Adversarial Adaptation for Monocular Depth Estimation from Thermal Image</p>
            <p class="authors"> <b>Ukcheol Shin</b>, Kwanyong Park, Kyunghyun Lee, Byeong-Uk Lee, In So Kweon</p>
            <p class="venue"> Machine Vision and Applications (<b>MVA</b>), 2023 <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://trebuchet.public.springernature.app/get_content/e16a1791-33c8-4ae1-a883-e686f0a69f8d"> paper /
                <a href="https://github.com/UkcheolShin/SelfDepth4Thermal"> code</a> 
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2023_CVPR_SupThermalDepth.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Deep Depth Estimation from Thermal Image</p>
            <p class="authors"> <b>Ukcheol Shin</b>, Jinsun Park, In So Kweon</p>
            <p class="venue"> Proceedings of the IEEE conference on computer vision and pattern recognition (<b>CVPR</b>), 2023 <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_Deep_Depth_Estimation_From_Thermal_Image_CVPR_2023_paper.pdf"> paper</a> /
                <a href="https://github.com/UkcheolShin/SupDepth4Thermal"> Benchmark</a> /
                <a href="https://sites.google.com/view/multi-spectral-stereo-dataset/home"> MS2 dataset</a> /
                <a href="https://github.com/UkcheolShin/MS2-MultiSpectralStereoDataset"> Github</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2023_wacv23_teaser.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Self-supervised Monocular Depth Estimation from Thermal Images via Adversarial Multi-spectral Adaptation</p>
            <p class="authors"> <b>Ukcheol Shin</b>, Kwanyong Park, Byeong-Uk Lee, Kyunghyun Lee, In So Kweon</p>
            <p class="venue"> IEEE Winter Conference on Applications of Computer Vision  (<b>WACV</b>), 2023 <span style="font-weight:bolder;color:#BB2222">Oral</span> <span style="font-weight:normal"> </span></p>                      
            <p class="venue"> <span style="font-weight:bolder;color:#BB2222">Best Student Paper Award</span>, Out of 641 accepted papers (out of 1577 submissions) <span style="font-weight:normal"> </span></p>                      
            <p class="resources">
              [
                <a href="https://openaccess.thecvf.com/content/WACV2023/html/Shin_Self-Supervised_Monocular_Depth_Estimation_From_Thermal_Images_via_Adversarial_Multi-Spectral_WACV_2023_paper.html"> paper /</a>
                <a href="https://github.com/UkcheolShin/SelfDepth4Thermal">  code</a> /
                <a href="https://drive.google.com/file/d/12o_jFgAMfxUAe46l7mqz8EOyct36SM9Q/view?usp=share_link"> poster</a> / 
                <a href="https://drive.google.com/file/d/11sdV9xkZ3Ln5VBch5G8AF5bgIgp_B8xp/view?usp=share_link"> award</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2022_RAL_ThermalMonoDepth.gif" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Maximizing Self-supervision from Thermal Image for Effective Self-supervised Learning of Depth and Ego-motion</p>
            <p class="authors"> <b>Ukcheol Shin</b>, Kyunghyun Lee, Byeong-Uk Lee, In So Kweon</p>
            <p class="venue"> IEEE Robotics and Automation Letters (<b>RA-L</b>), 2022  <span style="font-weight:normal"></span></p>
            <p class="venue"> IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2022 <span style="font-weight:bolder;color:#BB2222">Oral</span> <span style="font-weight:normal"> </span></p>            
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2201.04387">paper</a> /
                <a href="https://github.com/UkcheolShin/ThermalMonoDepth">code</a> /
                <a href="https://sites.google.com/view/thermal-monodepth-project-page">project</a> 
              ]
            </p>
          </div>
        </li>
          
          <li>
            <div class="img">
              <img src="/papers/images/2021_iccv_DRL_ISP2.gif" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">DRL-ISP: Multi-Objective Camera ISP with Deep Reinforcement Learning</p>
            <p class="authors"><b>Ukcheol Shin*</b>, Kyunghyun Lee*, In So Kweon (* equal contribution)</p>
            <p class="venue"> IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2022 <span style="font-weight:bolder;color:#BB2222">Oral</span> <span style="font-weight:normal"> </span></p>            
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2207.03081">paper</a> /
                <a href="https://github.com/UkcheolShin/DRL-ISP">code</a> /
                <a href="https://sites.google.com/view/drl-isp">project</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2022_cvpr_uda_cope.gif" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">UDA-COPE: Unsupervised Domain Adaptation for Category-level Object Pose Estimation</p>
            <p class="authors"> Taeyeop Lee, Byeong-Uk Lee, Inkyu Shin, Jaesung Choe, <b>Ukcheol Shin</b>, In So Kweon, Kuk-Jin Yoon</p>
            <p class="venue"> Proceedings of the IEEE conference on computer vision and pattern recognition (<b>CVPR</b>), 2022 <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2111.12580">paper</a> /
                <a href="https://sites.google.com/view/taeyeop-lee/udacope">project</a>

              ]
            </p>
          </div>
        </li>
        <li>
            <div class="img">
              <img src="/papers/images/2021_Tsfm_teaser3.gif" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title"> Self-supervised Depth and Ego-motion Estimation for Monocular Thermal Video using Multi-spectral Consistency Loss</p>
            <p class="authors"><b>Ukcheol Shin</b>, Kyunghyun Lee, Seokju Lee, In So Kweon</p>
            <p class="venue"> IEEE Robotics and Automation Letters (<b>RA-L</b>), 2021  <span style="font-weight:normal"></span></p>
            <p class="venue"> IEEE International Conference on Robotics and Automation (<b>ICRA</b>), 2022 <span style="font-weight:bolder;color:#BB2222">Oral</span> <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://ieeexplore.ieee.org/abstract/document/9662239">paper</a> / 
                <a href="https://github.com/UkcheolShin/ThermalSfMLearner-MS">code</a>  /
                <a href="https://sites.google.com/view/t-sfmlearner">project</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2021_ral_MSGDAT.gif" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">MS-UDA: Multi-Spectral Unsupervised Domain Adaptation for Thermal Image Semantic Segmentation</p>
            <p class="authors"> YeongHyeon Kim, <b>Ukcheol Shin</b>, Jinsun Park, In So Kweon</p>
            <p class="venue"> IEEE Robotics and Automation Letters (<b>RA-L</b>), 2021  <span style="font-weight:normal"></span></p>
            <p class="venue"> IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2021 <span style="font-weight:bolder;color:#BB2222">Oral</span> <span style="font-weight:normal"> </span></p>
            <p class="resources">
              [
                <a href="https://ieeexplore.ieee.org/document/9468936">paper</a> /
                <a href="https://github.com/yeong5366/MS-UDA">code</a> /
                <a href="https://github.com/yeong5366/MS-UDA">project</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2020_nips.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">An Efficient Asynchronous Method for Integrating Evolutionary and Gradient-based Policy Search</p>
            <p class="authors">Kyunghyun Lee, Byeong-Uk Lee, <b>Ukcheol Shin</b>, In So Kweon</p>
            <p class="venue"> Conference on Neural Information Processing Systems (<b>NeurIPS</b>), 2020 <span style="font-weight:bolder;color:#BB2222">Oral</span> <span style="font-weight:normal">(Acceptance: 105/9454 ≈ 1.1%) <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://arxiv.org/abs/2012.05417">paper</a> /
                <a href="https://github.com/KyunghyunLee/aes-rl">code</a> 
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2019_IROS19_sesnor.png" class="img-responsive" alt="" >
            </div>
          <div class="description">
            <p class="title">Vehicular multi-camera sensor system for automated visual inspection of electric power distribution equipment</p>
            <p class="authors">Jinsun Park, <b>Ukcheol Shin</b>, Gyumin Shim, Kyungdon Joo, Francois Rameau, Junhyeok Kim, Dong-Geol Choi, In So Kweon</p>
            <p class="venue">IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2019 <span style="font-weight:bolder;color:#BB2222">Oral</span> <span style="font-weight:normal"> <span style="font-weight:normal"></span></p>
            <p class="venue"> <span style="font-weight:bolder;color:#BB2222">Media coverage: covered by <a href="https://mbnmoney.mbn.co.kr/news/print_view?news_no=MM1004382130"> MBN</a>, <a href="https://www.energydaily.co.kr/news/articleView.html?idxno=119772"> EnergyDaily</a>, and many local media </span> <span style="font-weight:normal"> </span></p>
            <p class="resources">
              [
                <a href="https://ieeexplore.ieee.org/abstract/document/8968085">paper</a>
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2019_IROS19_AE.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Camera exposure control for robust robot vision with noise-aware image quality assessment</p>
            <p class="authors"><b>Ukcheol Shin</b>, Jinsun Park, Gyumin Shim, Francois Rameau, In So Kweon</p>
            <p class="venue">IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>), 2019 <span style="font-weight:bolder;color:#BB2222">Oral</span> <span style="font-weight:normal"> <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://ieeexplore.ieee.org/abstract/document/8968590">paper</a> / 
                <a href="https://github.com/UkcheolShin/Noise-AwareCameraExposureControl">dataset</a> /
                <a href="https://github.com/UkcheolShin/Noise-AwareCameraExposureControl">code</a>  /
                <a href="https://sites.google.com/view/noise-aware-exposure-control/%ED%99%88">project</a> 
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2017_RTOS.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Performance evaluation of real-time mechanisms on open embedded hardware platforms</p>
            <p class="authors"><b>Ukcheol Shin</b>, Byoung Wook Choi</p>
            <p class="venue"> Journal of Institute of Control, Robotics and Systems, 2017  <span style="font-weight:normal"></span></p>
            <p class="resources">
              [
                <a href="https://www.semanticscholar.org/paper/Performance-Evaluation-of-Real-time-Mechanisms-on-Shin-Choi/c7b64944583a21452008f0c41f3d17452eaad646?p2df">paper</a> 
              ]
            </p>
          </div>
        </li>

        <li>
            <div class="img">
              <img src="/papers/images/2016_RTOS.png" class="img-responsive" alt="">
            </div>
          <div class="description">
            <p class="title">Development and control of an omnidirectional mobile robot on an ethercat network</p>
            <p class="authors">Raimarius Delgado, <b>UkCheol Shin</b>, Chang Hwi Hong, Byoung Wook Choi</p>
            <p class="venue">International Journal of Applied Engineering Research, 2016 </p>
            <p class="resources">
              [
                <a href="https://www.researchgate.net/profile/Raimarius-Delgado/publication/310461468_Development_and_Control_of_an_Omnidirectional_Mobile_Robot_on_an_EtherCAT_Network/links/582e628308aef19cb813e6ff/Development-and-Control-of-an-Omnidirectional-Mobile-Robot-on-an-EtherCAT-Network.pdf">paper</a>
              ]
            </p>
          </div>
        </li>


        </ul>      
    </div>  
</div>
    </div>
  </section>

<section id="awards" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">

<div class="row">  
    <div class="col-lg-12">
      <h1>Awards &amp; Honors</h1>      
<ul>
<li><b>1st Place Award</b>, CVPR 2024 Autonomous Driving Workshop "Argoverse Scene Flow Challenge", June 2024</li>
<li><b>Honorable Mention</b>, 29th HumanTech Paper Award, Samsung Electronics Co., Ltd. Feb. 2023 ($2,000)</li>
<li><span style="font-weight:bolder;color:#BB2222">Best Student Paper Award</span>, IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), Jan. 2023</li>
<li><b>Graduation with Honors (Top 3%)</b>, SNUST in Electrical and Information Engineering, Feb. 2017</li>
<li><b>Honorable Mention</b>, All-semester Design Based Learning (ADBL) Capstone Contest, June 2016</li>
<li><b>Second Prize</b>, Robot Open Academy, Feb. 2016</li>
<li><b>Grand Prize</b>, All-semester Design Based Learning (ADBL) Capstone Contest, Dec. 2015</li>
<li><b>Military Service</b>, Army Sergeant, Honorable discharge, Feb. 2013 - Nov. 2014</li>
<li><b>Scholarship for Academic Excellence</b>, Scholarship for the entire B.S. program (2011-2012, 2015-2016)</li>
</ul>
    </div>
</div>
    </div>
  </section>


<section id="activities" class="home-section wg-blank   " style="padding: 20px 0 20px 0;" >
    <div class="container">

<div class="row">  
    <div class="col-lg-12">
      <h1>Academic Activities</h1>      
      <h2 id="reviewer">Journal Reviewer</h2>
<ul>
<li>IEEE Transactions on Cybernetics (<b>T-CYB</b>)(2022)</li>
<li>IEEE Transactions on Intelligent Vehicles (<b>T-IV</b>)(2023-2024)</li>
<li>IEEE Transactions on Intelligent Transportation Systems (<b>T-ITS</b>)(2025)</li>
<li>IEEE Robotics and Automation Letters (<b>RA-L</b>) (2022-2025)</li>
<li>IEEE Transactions on Neural Networks and Learning System (<b>T-NNLS</b>)(2024-2025)</li>
<li>ACM Transactions on Multimedia Computing, Communications, and Applications (<b>T-OMM</b>)(2024)</li>
<li>Neural Processing Letters (2023)</li>
<li>Pattern Recognition (<b>PR</b>) (2023)</li>
</ul>
      <h2 id="reviewer">Conference Reviewer</h2>
<ul>
<li>IEEE International Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>) (2022-Now)
<li>IEEE International Conference on Computer Vision (<b>ICCV</b>) (2021,2023,2025)
<li>European Conference on Computer Vision (<b>ECCV</b>) (2024)
<li>IEEE/CVF Winter Conference on Applications of Computer Vision (<b>WACV</b>) (2023) </li>
<li>IEEE International Conference on Robotics and Automation (<b>ICRA</b>) (2024-Now) </li>
<li>Robotics: Science and Systems (<b>RSS</b>) (2024-Now) </li>
<li>Conference on Neural Information Processing Systems (<b>NeurIPS</b>) (2021-Now) </li>
<li>International Conference on Machine Learning (<b>ICML</b>) (2022-Now) </li>
<li>International Conference on Learning Representations(<b>ICLR</b>) (2022-Now) </li>
<li>AAAI Conference on Artificial Intelligence (<b>AAAI</b>) (2021-Now) </li>
</ul>
      <h2 id="reviewer">Program Committee</h2>
<ul>
<li>Organizer, ICRA 2025 Workshop on <a href="https://sites.google.com/view/tiro25/" style="color: black; text-decoration: underline;">'Thermal Infrared in Robotics'</a>, May 2025</li>
<li>Session Chair, ICRA 2025 Session on “Representation Learning 2”, May 2025</li>
<li>Organizer, ICCV 2025 Workshop on <a href="https://multispectral4ra.github.io/" style="color: black; text-decoration: underline;">'Multispectral Imaging for Robotics and Automation'</a>, Oct. 2025</li>
</ul>

      <h2 id="reviewer">Invited Talks</h2>
<ul>
<li>ACCV 2024 Workshop on <a href="https://multispectral4ra.github.io/">'Multispectral Imaging for Robotics and Automation'</a>, Nov. 2024</li>
<li>KAIST (Korea Advanced Institute of Science and Technology), Sep. 2024</li>
<li>Hanyang University, May 2024</li> 
<li>SNU (Seoul National University), Feb. 2024</li>
<li>GIST (Gwangju Institute of Science and Technology), Sep. 2023</li>
<li>UNIST (Ulsan National Institute of Science and Technology), July 2023</li>
<li>Pusan National University, July 2023</li>
</ul>
    </div>
</div>
    </div>
  </section>
  
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script> 
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    <script>const code_highlighting = false;</script>
    <script>const isSiteThemeDark = false;</script>
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>

    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>

    <script src="/js/academic.min.a8d7005002cb4a052fd6d721e83df9ba.js"></script>


  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    
    .
    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
